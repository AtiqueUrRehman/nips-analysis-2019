{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "from tqdm import tqdm\n",
    "from os import path\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrol_infinite(driver, scrool_pause_time = 1):\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(scrool_pause_time)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "        \n",
    "def download_all_videos(extracted_links, heading_list):\n",
    "    \"\"\"\n",
    "    Download all videos and save in data dir\n",
    "    \"\"\"\n",
    "    for i in tqdm(range(len(extracted_links))):\n",
    "        request.urlretrieve(extracted_links[i], path.join(\"../data\", heading_list[i]))\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of Presentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://slideslive.com/neurips\")\n",
    "scrol_infinite(driver)\n",
    "html = driver.page_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html)\n",
    "presentations_div = soup.find('div',attrs={'class':'presentationsGrid'})\n",
    "list_presentations_pages = []\n",
    "list_presentations = presentations_div.find_all('div', attrs={'data-item-type':'presentation'})\n",
    "for presentation in list_presentations:\n",
    "    url = presentation.find('a')['href']\n",
    "    list_presentations_pages.append(url)\n",
    "    \n",
    "print (len(list_presentations_pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save presentations list to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/all_urls.txt', 'w') as f:\n",
    "    for item in list_presentations_pages:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visit each presentation and get video link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_links = []\n",
    "heading_list = []\n",
    "for presentation_link in list_presentations_pages:\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(presentation_link)\n",
    "    heading = driver.find_element_by_tag_name('h1').get_attribute('innerHTML')\n",
    "    heading = heading.strip().replace(\"amp;\", '_').replace(\" \", \"_\")\n",
    "\n",
    "    iframe_div = driver.find_element_by_class_name('slp__videoPlayer__content')\n",
    "    iframe = iframe_div.find_element_by_tag_name('iframe')\n",
    "\n",
    "    driver.switch_to.frame(iframe)\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html)\n",
    "    script = soup.find_all('script')[4]\n",
    "    res = re.search('\"url\":\"(https://gcs-vimeo.akamaized.net.*?.mp4)\"', script.string)\n",
    "    print (res.group(1), heading)\n",
    "\n",
    "    extracted_links.append(res.group(1))\n",
    "    heading_list.append(heading)\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save names and links to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_urls_headings.txt', 'w') as f:\n",
    "    for i in range(len(extracted_links)): \n",
    "        f.write(\"%s %s\\n\" % (heading_list[i], extracted_links[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download all videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/261 [04:05<17:44:06, 245.56s/it]"
     ]
    }
   ],
   "source": [
    "download_all_videos(extracted_links, heading_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
